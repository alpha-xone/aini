defaults:
  human_input_mode: TERMINATE
  default_auto_reply: ''

conversable:
  # A class for generic conversable agents which can be configured as assistant or user proxy.
  class: autogen.UserProxyAgent
  params:
    # str: name of the agent.
    name: ${name}
    # function: a function that takes a message in the form of a dictionary
    #   and returns a boolean value indicating if this received message is a termination message.
    #   The dict can contain the following keys: "content", "role", "name", "function_call".
    is_termination_msg: ${is_termination_msg}
    # int: the maximum number of consecutive auto replies.
    #   Default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case).
    #   When set to 0, no auto reply will be generated.
    max_consecutive_auto_reply: ${max_consecutive_auto_reply}
    # str: whether to ask for human inputs every time a message is received.
    #   Possible values are "ALWAYS", "TERMINATE", "NEVER".
    #   (1) When "ALWAYS", the agent prompts for human input every time a message is received.
    #       Under this mode, the conversation stops when the human input is "exit",
    #       or when is_termination_msg is True and there is no human input.
    #   (2) When "TERMINATE", the agent only prompts for human input only when a termination message is received or
    #       the number of auto reply reaches the max_consecutive_auto_reply.
    #   (3) When "NEVER", the agent will never prompt for human input. Under this mode, the conversation stops
    #       when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.
    human_input_mode: ${human_input_mode}
    # dict[str, callable]: mapping function names (passed to openai) to callable functions, also used for tool calls.
    function_map: ${function_map}
    # dict or False: config for the code execution.
    # To disable code execution, set to False. Otherwise, set to a dictionary with the following keys:
    # - work_dir (Optional, str): The working directory for the code execution.
    #     If None, a default working directory will be used.
    #     The default working directory is the "extensions" directory under
    #     "path_to_autogen".
    # - use_docker (Optional, list, str or bool): The docker image to use for code execution.
    #     Default is True, which means the code will be executed in a docker container. A default list of images will be used.
    #     If a list or a str of image name(s) is provided, the code will be executed in a docker container
    #     with the first image successfully pulled.
    #     If False, the code will be executed in the current environment.
    #     We strongly recommend using docker for code execution.
    # - timeout (Optional, int): The maximum execution time in seconds.
    # - last_n_messages (Experimental, int or str): The number of messages to look back for code execution.
    #     If set to 'auto', it will scan backwards through all messages arriving since the agent last spoke, which is typically the last time execution was attempted. (Default: auto)
    code_execution_config: ${code_execution_config}
    # LLMConfig or dict or False: llm inference configuration.
    #   Please refer to [OpenAIWrapper.create](https://docs.ag2.ai/latest/docs/api-reference/autogen/OpenAIWrapper/#autogen.OpenAIWrapper.create)
    #   for available options.
    #   When using OpenAI or Azure OpenAI endpoints, please specify a non-empty 'model' either
    #   in `llm_config` or in each config of 'config_list' in `llm_config`.
    #   To disable llm-based auto reply, set to False.
    #   When set to None, will use self.DEFAULT_CONFIG, which defaults to False.
    llm_config: ${llm_config}
    # str or dict: default auto reply when no code execution or llm-based reply is generated.
    default_auto_reply: ${default_auto_reply}
    # str or list: system message for the ChatCompletion inference.
    system_message: ${system_message}
    # str: a short description of the agent. This description is used by other agents
    #   (e.g. the GroupChatManager) to decide when to call upon this agent.
    #   (Default: system_message)
    description: ${description}
    # dict: the previous chat messages that this agent had in the past with other agents.
    #   Can be used to give the agent a memory by providing the chat history. This will allow
    #   the agent to resume previous had conversations. Defaults to an empty chat history.
    chat_messages: ${chat_messages}
    # bool: (Experimental) whether to print the message sent. If None, will use the value of
    #   silent in each function.
    silent: ${silent}
    # ContextVariables: Context variables that provide a persistent context for the agent.
    #   Note: This will be a reference to a shared context for multi-agent chats.
    #   Behaves like a dictionary with keys and values (akin to dict[str, Any]).
    context_variables: ${context_variables}
    # List[Callable[..., Any]]: a list of functions to register with the agent,
    #   these will be wrapped up as tools and registered for LLM (not execution).
    functions: ${functions}
    # List[Callable[..., Any]]: a list of functions, including UpdateSystemMessage's,
    #   called to update the agent before it replies.
    update_agent_state_before_reply: ${update_agent_state_before_reply}
    # Handoffs: handoffs object containing all handoff transition conditions.
    handoffs: ${handoffs}
